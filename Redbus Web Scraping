import pyodbc
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# Database connection details
server = 'localhost\SQLEXPRESS' 
database = 'BusDetails'          
username = 'sa'                 
password = '123'                

# SQL Server connection
def connect_to_sql():
    connection_string = f'Driver={{SQL Server}};Server={server};Database={database};UID={username};PWD={password}'
    connection = pyodbc.connect(connection_string)
    return connection

#Bus details Insertion into SQL database
def insert_bus_details(bus_details):
    connection = connect_to_sql()
    cursor = connection.cursor()

    insert_query = """
    INSERT INTO RedBus_BusDetails (State_Name, Route_Name, Route_Link, Bus_Name, Bus_Type, Departing_Time, Duration, Reaching_Time, Star_Rating, Price, Seat_Availability)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """
    
    for bus in bus_details:
        cursor.execute(insert_query, (
            bus["State_Name"],
            bus["Route_Name"],
            bus["Route_Link"],
            bus["Bus_Name"],
            bus["Bus_Type"],
            bus["Departing_Time"],
            bus["Duration"],
            bus["Reaching_Time"],
            bus["Star_Rating"],
            bus["Price"],
            bus["Seat_Availability"]
        ))

    # Commit and close the connection
    connection.commit()
    cursor.close()
    connection.close()

# Initialize WebDriver
def initialize_driver():
    driver = webdriver.Chrome()
    driver.maximize_window()
    return driver

# Load the page
def load_page(driver, url):
    driver.get(url)
    time.sleep(5) 

# Scrape Bus Routes
def scrape_bus_routes(driver):
    route_elements = driver.find_elements(By.CLASS_NAME, 'route')
    bus_routes_link = [route.get_attribute('href') for route in route_elements]
    bus_routes_name = [route.text.strip() for route in route_elements]
    return bus_routes_link, bus_routes_name

# Scrape bus details
def scrape_bus_details(driver, url, route_name, state_name):
    try:
        driver.get(url)
        time.sleep(5)
        
        try:
            view_buses_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CLASS_NAME, "button"))
            )
            driver.execute_script("arguments[0].click();", view_buses_button)
            time.sleep(5)

            # Scroll down to load all bus
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(5)

            # Find Bus Item Details
            bus_name_elements = driver.find_elements(By.CLASS_NAME, "travels.lh-24.f-bold.d-color")
            bus_type_elements = driver.find_elements(By.CLASS_NAME, "bus-type.f-12.m-top-16.l-color.evBus")
            departing_time_elements = driver.find_elements(By.CLASS_NAME, "dp-time.f-19.d-color.f-bold")
            duration_elements = driver.find_elements(By.CLASS_NAME, "dur.l-color.lh-24")
            reaching_time_elements = driver.find_elements(By.CLASS_NAME, "bp-time.f-19.d-color.disp-Inline")
            star_rating_elements = driver.find_elements(By.XPATH, "//div[@class='rating-sec lh-24']")
            price_elements = driver.find_elements(By.CLASS_NAME, "fare.d-block")

            seat_availability_elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'seat-left m-top-30') or contains(@class, 'seat-left m-top-16')]")

            bus_details = []
            for i in range(len(bus_name_elements)):
                bus_detail = {
                    "State_Name": state_name,
                    "Route_Name": route_name,
                    "Route_Link": url,
                    "Bus_Name": bus_name_elements[i].text,
                    "Bus_Type": bus_type_elements[i].text,
                    "Departing_Time": departing_time_elements[i].text,
                    "Duration": duration_elements[i].text,
                    "Reaching_Time": reaching_time_elements[i].text,
                    "Star_Rating": star_rating_elements[i].text if i < len(star_rating_elements) else '0',
                    "Price": price_elements[i].text,
                    "Seat_Availability": seat_availability_elements[i].text if i < len(seat_availability_elements) else '0'
                }
                bus_details.append(bus_detail)
            return bus_details
        
        except Exception as e:
            print(f"Error occurred while scraping bus details for {url}: {str(e)}")
            return [] 

    except Exception as e:
        print(f"Error occurred while accessing {url}: {str(e)}")
        return [] 

# Scrape all pages (with pagination)
def scrape_all_pages():
    for page in range(1, 3):  # Adjust the range to scrape multiple pages
        try:
            driver = initialize_driver()
            load_page(driver, "https://www.redbus.in/online-booking/ksrtc-kerala/?utm_source=rtchometile") #Replace State Name
            
            if page > 1:
                pagination_tab = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, f"//div[contains(@class, 'DC_117_pageTabs') and text()='{page}']"))
                )
                
                driver.execute_script("arguments[0].scrollIntoView();", pagination_tab)
                driver.execute_script("arguments[0].click();", pagination_tab)
                time.sleep(5)  
                
            all_bus_routes_link, all_bus_routes_name = scrape_bus_routes(driver)
            for link, name in zip(all_bus_routes_link, all_bus_routes_name):
                bus_details = scrape_bus_details(driver, link, name, "Kerala")  # Replace with the correct state name
                if bus_details:
                    insert_bus_details(bus_details)
                
        except Exception as e:
            print(f"Error occurred while scraping page {page}: {str(e)}")
        finally:
            driver.quit()  

scrape_all_pages()
